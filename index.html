<!DOCTYPE html>
<html>
<head>
	<meta charset='utf-8'/>
	<title>Adventures in Deep Learning – Nate Parrott</title>
	<link rel='stylesheet' href='style.css'/>
	<meta name="viewport" content="width=device-width, initial-scale=1" />
</head>
<body>
	<section id='title'>
		<h1><span>Adventures in</span> <strong>Deep Learning</strong></h1>
		<p id='by'><a href='http://portfolio.nateparrott.com'>by Nate Parrott</a></p>
		<p>Over the past couple months, I've been trying to learn more about neural networks and deep learning by implementing various algorithms myself. Here's what I've been up to:</p>
	</section>
	
	<section>
		<div class='desc-and-figure'>
			<div class='desc'>
				<h2><a href='https://github.com/nate-parrott/juypter-notebooks/blob/master/cifar-model.ipynb'>Recognizing real-world images</a> — 75% accuracy on CIFAR-10 using convolutional networks</h2>
				<p>I loosely followed the <a href='https://www.tensorflow.org/tutorials/deep_cnn/'>TensorFlow CIFAR-10 tutorial</a>, using a simple convolutional neural network to classify 32x32 images into one of 10 categories (cats, deer, airplanes, etc). Running on a GPU for less than an hour, it achieved a pretty decent accuracy.</p>
			</div>
			
			<figure class='cifar-dog slanted'>
				<img src='cifar-dog.png'/>
				<figcaption>An image and the network's predictions of what it might be. (the image was downsampled to 32x32 before being fed into the network)</figcaption>
			</figure>
		</div>
		
		<figure class='centered'>
			<img src='cifar-examples.png' style='max-width: 332px'/>
			<figcaption>A few of the 60,000 images from CIFAR-10 – 32x32-pixel images labelled with one of ten categories.</figcaption>
		</figure>
	</section>
	
	<section>
		<div class='desc'>
			<h2><a href='https://arxiv.org/abs/1508.06576'>Neural style transfer</a></h2>
			<p>Using a pre-trained network trained to classify images from the ImageNet dataset, I implemented the <a href='https://arxiv.org/abs/1508.06576'>Neural Algorithm for Artistic Style</a> paper (which is similar to what's used by the photo-filtered algorithm <a href='http://prisma-ai.com/'>Prisma</a>, but isn't the same). The algorithm's underlying assumption is that the activations of earlier layers of a convolutional neural network represent the "stylistic" aspect of an image — the details of strokes, colors used, etc; and that the later, higher-level layers represent the high-level "content" of an image. I used gradient descent to generate an image that where low-level feature activations were similar to activations from the "style image," while the higher-level feature activations stayed similar to the "content" image.</p>
		</div>
		<figure class='wide'>
			<img src='xfer.png'/>
			<figcaption>A "content" image, a "style" image, and the mixture of both images using style transfer.</figcaption>
		</figure>
	</section>
	
	<section>
		<div class='desc'>
			<h2><a href='https://github.com/nate-parrott/juypter-notebooks/blob/master/mnist-variational-autoencoder.ipynb'>Generating fake handwritten MNIST digit</a> using variational autoencoders</h2>
			<p>
				I'm interested in generative models that use neural networks to produce <em>new</em> content. I took the MNIST dataset and trained a <a href='http://kvfrans.com/variational-autoencoders-explained/'>variational autoencoder</a> — a neural network that tries to compress its input image into a vector of numbers and then reconstruct it. You can do arithmetic on these latent vectors — for example, "blending" two digits together by averaging their vectors. Latent vectors can also be sampled from randomly to generate random digit-like images.
			</p>
			<figure class='wide'>
				<img src='reconstructed.png'/>
				<figcaption>Some original MNIST digits (top) and the network's reconstruction of them, after compressing them down to vectors of 10 floating-point numbers.</figcaption>
			</figure>
			<figure class='wide'>
				<img src='blended.png'/>
				<figcaption>A 0 is transformed into a 2 by taking weighted sums of their vectors, then running the "reconstruction" step to produce an image.</figcaption>
			</figure>
			<figure class='wide'>
				<img src='mnist_generated.png'/>
				<figcaption>Some images taken by randomly generating latent vectors and reconstructing an image from them.</figcaption>
			</figure>
	</section>
	
	<section>
		<div class='desc-and-figure'>
			<figure class='slanted left'>
				<img src='mnist.png' style='max-width: 260px'/>
			</figure>
			
			<div class='desc'>
				<h2><a href='https://github.com/nate-parrott/juypter-notebooks/blob/master/mnist.ipynb'>Handwritten digit recognition</a> — 99% accuracy on MNIST with convolutional networks</h2>
				<p>Classifying handwritten digits 0–9, using the <a href='http://yann.lecun.com/exdb/mnist/'>MNIST dataset</a>, as part of the <a href='http://cs.brown.edu/courses/csci2950-k/'>CS2950k</a> course at Brown University. We started by writing a <a href='https://github.com/nate-parrott/cs2950k/blob/master/hw1.py'>fully-connected multi-layer neural network without using a deep learning framework,</a> implementing backpropogation manually with numpy. Then, we built a <a href='https://github.com/nate-parrott/cs2950k/blob/master/hw2.py'>fully-connected network in Tensorflow</a>. Finally, following the <a href='https://www.tensorflow.org/tutorials/mnist/pros/'>Deep MNIST tutorial</a> on the Tensorflow website, we build a <a href='https://github.com/nate-parrott/juypter-notebooks/blob/master/mnist.ipynb'>convolutional network</a> that achieved 99% accuracy on the test dataset.</p>
			</div>
		</div>
	</section>
	
	<section>
		<div class='desc'>
			<figure class='slanted float'>
				<img src='mnist-gan.png' style='max-width: 260px'/>
			</figure>
			<h2><a href='https://github.com/nate-parrott/deepnets/blob/master/mnist-gan-tuned.py'>Generating plausible, fake MNIST digits</a> with deep convolutional generative adversarial networks.</h2>
			<p>
				Deep convolutional generative adversarial networks (DCGANs) are a really interesting approach to the problem of generative "plasusible fakes" that <em>look like</em> something that could have come from a real-world dataset, but are actually randomly generated. DCGANs are essentially <em>two networks</em> — a "generator" that generates images, and a "discriminator" that is trained to guess whether an image comes from the real dataset, or wwas generated by the generator. The two are alternatingly trained — when the generator is being trained, error is <em>backpropogated through the disciminator first</em>, trying to maximize the featues that cause the discriminator to think the generator's output is real.
			</p>
			<p>
				The DCGAN proved difficult to train — the learning rate needed to be fine-tuned to prevent the generator from collapsing to a single output image. The discriminator seemed to learn much faster than the generator — the discriminator would reach 90% accuracy quickly, while the generator would take 10x more steps to get good enough at "fooling" the discriminator to bring the discriminator accuracy back down to 50%.
			</p>
			<p>
				I'd like to try using generative adversarial models on different images, like <a>ImageNet</a> or faces, or on audio or textual data. I'd also like to experiment with different architectures to make GANs easier to train — training multiple disciminators at once might help by making it less likely for the generator to overfit its output to the particular discriminator's quirks.
			</p>
		</div>
	</section>
	
	<section>
		<div class='desc'>
			<figure class='shadowed float' style='max-width: 260px'>
				<img src='frames.gif'/>
				<figcaption>A neural network playing the "falling rocks" game using policy gradients. It isn't very good, but it achieves a 5x-higher score than an AI making random moves.</figcaption>
			</figure>
			
			<h2><a href='https://github.com/nate-parrott/deepnets/blob/master/rocksnet5.py'>Playing a simple game</a> with deep reinforcement learning</h2>
			<p>
				<a href='https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf'>DeepMind</a> stunned the machine-learning community in 2013 by announcing that it had built a single neural network that could learn to play any of 15 Atari games, taking as input only the raw pixels on the screen and a "reward" value that told it when it scored a point. The computer "plays" the game <em>many thousands</em> of times, and is trained to take a game-screen image and output the action that's most likely to lead to rewards down the line.
			</p>
			<p>
				Rather than an Atari game, I wrote a <em>very simple</em> game for the computer to play — "rocks" fall from random positions at the top of the screen, and the player moves their "paddle" horizontally to catch them. Teaching a network to play the game was more difficult than I expected. Using <a href='https://www.nervanasys.com/demystifying-deep-reinforcement-learning/'>Deep Q-Learning</a> like Deepmind did, I was able to train a pretty good AI to play the 8-by-8-square version of the game — but I had trouble training one to be successful on the 16x16 version. I switched to using <a heef='http://karpathy.github.io/2016/05/31/rl/'>policy gradients</a> — a slightly simpler approach — and was able to train a successful player on the 16x16 version. Unlike the Deepmind paper, neither of these networks were convolutional — convolutional versions of these networks didn't seem to converge quickly, and I'm not sure the game was complex enough for them to have had any benefit.
			</p>
		</div>
	</section>
</body>
</html>
